{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/bee-wasp-data/data.zip\n",
    "# !unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import models, layers, optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"data/train\"\n",
    "test_folder = \"data/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next, create a convolutional layer (Conv2D):\n",
    "Use 32 filters\n",
    "Kernel size should be (3, 3) (that's the size of the filter)\n",
    "Use 'relu' as activation\n",
    "\"\"\"\n",
    "conv_2d_layer = layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=(150, 150, 3))\n",
    "model.add(conv_2d_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reduce the size of the feature map with max pooling (MaxPooling2D)\n",
    "Set the pooling size to (2, 2)\n",
    "\"\"\"\n",
    "max_pooling_2d_layer = layers.MaxPooling2D((2,2))\n",
    "model.add(max_pooling_2d_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Turn the multi-dimensional result into vectors using a Flatten layer\"\"\"\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "\"\"\"Next, add a Dense layer with 64 neurons and 'relu' activation\"\"\"\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "\"\"\"\n",
    "Finally, create the Dense layer with 1 neuron - this will be the output\n",
    "The output layer should have an activation - use the appropriate activation \n",
    "for the binary classification case.\n",
    "\"\"\"\n",
    "# Appropriate activation for binary classification is \"sigmoid\"\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As optimizer use SGD with the following parameters:\n",
    "SGD(lr=0.002, momentum=0.8)\n",
    "\"\"\"\n",
    "sgd_optimizer = optimizers.SGD(learning_rate=0.002, momentum=0.8)\n",
    "loss_for_binary_class = \"binary_crossentropy\"\n",
    "\n",
    "model.compile(\n",
    "    loss=loss_for_binary_class,\n",
    "    optimizer=sgd_optimizer,\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Since we have a binary classification problem, what is the best loss function for us?\n",
    "\n",
    "### Answer to Q1:\n",
    "- binary crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 74, 74, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 175232)            0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                11214912  \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11215873 (42.79 MB)\n",
      "Trainable params: 11215873 (42.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "What's the number of parameters in the convolutional layer of our model? You can use the summary method for that.\n",
    "\n",
    "### Answer to Q2:\n",
    "- 11214912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3677 images belonging to 2 classes.\n",
      "Found 918 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For the next two questions, use the following data generator for both train and test sets:\n",
    "ImageDataGenerator(rescale=1./255)\n",
    "We don't need to do any additional pre-processing for the images.\n",
    "\"\"\"\n",
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\"\"\"\n",
    "When reading the data from train/test directories, check the class_mode parameter.\n",
    "Which value should it be for a binary classification problem? (binary)\n",
    "Use batch_size=20\n",
    "Use shuffle=True for both training and test sets.\n",
    "\"\"\"\n",
    "common_params = {\n",
    "    \"target_size\": (150, 150),\n",
    "    \"batch_size\": 20,\n",
    "    \"shuffle\": True,\n",
    "    \"class_mode\": \"binary\",\n",
    "}\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    train_folder,\n",
    "    **common_params,\n",
    ")\n",
    "\n",
    "test_ds = test_gen.flow_from_directory(\n",
    "    test_folder,\n",
    "    **common_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "184/184 [==============================] - 34s 184ms/step - loss: 0.3025 - acc: 0.8888 - val_loss: 0.5115 - val_acc: 0.7778\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 34s 185ms/step - loss: 0.2595 - acc: 0.9094 - val_loss: 0.5380 - val_acc: 0.7549\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 34s 184ms/step - loss: 0.2246 - acc: 0.9230 - val_loss: 0.6100 - val_acc: 0.7353\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 34s 185ms/step - loss: 0.1871 - acc: 0.9451 - val_loss: 0.6778 - val_acc: 0.7135\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 35s 189ms/step - loss: 0.1544 - acc: 0.9578 - val_loss: 0.6124 - val_acc: 0.7266\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 35s 191ms/step - loss: 0.1365 - acc: 0.9657 - val_loss: 0.7452 - val_acc: 0.7298\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 31s 166ms/step - loss: 0.1109 - acc: 0.9758 - val_loss: 0.6300 - val_acc: 0.7702\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 37s 199ms/step - loss: 0.0856 - acc: 0.9818 - val_loss: 0.6519 - val_acc: 0.7603\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 36s 196ms/step - loss: 0.0766 - acc: 0.9891 - val_loss: 0.6639 - val_acc: 0.7723\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 36s 196ms/step - loss: 0.0571 - acc: 0.9910 - val_loss: 0.6413 - val_acc: 0.7767\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For training use .fit() with the following params:\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator\n",
    ")\n",
    "\"\"\"\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=test_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9617894887924194\n",
      "0.07820874997477685\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_acc = history.history[\"acc\"]\n",
    "training_loss = history.history[\"loss\"]\n",
    "print(np.median(training_acc))\n",
    "print(np.std(training_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "What is the median of training accuracy for all the epochs for this model?\n",
    "\n",
    "* 0.20\n",
    "* 0.40\n",
    "* 0.60\n",
    "* 0.80\n",
    "\n",
    "### Answer to Q3:\n",
    "- 0.80  (closest one to the accuracy I got - 0.96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "What is the standard deviation of training loss for all the epochs for this model?\n",
    "\n",
    "* 0.031\n",
    "* 0.061\n",
    "* 0.091\n",
    "* 0.131\n",
    "\n",
    "### Answer to Q4:\n",
    "- 0.091  (closest one to the loss I got - 0.078)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3677 images belonging to 2 classes.\n",
      "Found 918 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For the next two questions, we'll generate more data using data augmentations. \n",
    "\n",
    "Add the following augmentations to your training data generator:\n",
    "\n",
    "* `rotation_range=50,`\n",
    "* `width_shift_range=0.1,`\n",
    "* `height_shift_range=0.1,`\n",
    "* `zoom_range=0.1,`\n",
    "* `horizontal_flip=True,`\n",
    "* `fill_mode='nearest'`\n",
    "\"\"\"\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=50,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    ")\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    train_folder,\n",
    "    **common_params,\n",
    ")\n",
    "test_ds = test_gen.flow_from_directory(\n",
    "    test_folder,\n",
    "    **common_params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "184/184 [==============================] - 29s 155ms/step - loss: 0.5319 - acc: 0.7590 - val_loss: 0.4967 - val_acc: 0.7821\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 30s 163ms/step - loss: 0.4876 - acc: 0.7707 - val_loss: 0.5228 - val_acc: 0.7658\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 30s 165ms/step - loss: 0.4897 - acc: 0.7748 - val_loss: 0.4690 - val_acc: 0.7963\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 31s 169ms/step - loss: 0.4732 - acc: 0.7928 - val_loss: 0.4684 - val_acc: 0.7941\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 29s 155ms/step - loss: 0.4704 - acc: 0.7868 - val_loss: 0.5123 - val_acc: 0.7658\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 28s 152ms/step - loss: 0.4508 - acc: 0.8009 - val_loss: 0.5259 - val_acc: 0.7571\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 28s 153ms/step - loss: 0.4573 - acc: 0.8042 - val_loss: 0.5078 - val_acc: 0.7691\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 28s 152ms/step - loss: 0.4351 - acc: 0.8088 - val_loss: 0.4542 - val_acc: 0.8094\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 28s 153ms/step - loss: 0.4384 - acc: 0.8072 - val_loss: 0.4318 - val_acc: 0.8115\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 28s 151ms/step - loss: 0.4411 - acc: 0.8055 - val_loss: 0.4401 - val_acc: 0.7930\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's train our model for 10 more epochs using the same code as previously.\n",
    "> **Note:** make sure you don't re-create the model - we want to continue training the model\n",
    "we already started training.\n",
    "\"\"\"\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=test_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48288453817367555\n",
      "0.7880174279212951\n"
     ]
    }
   ],
   "source": [
    "validating_acc = history.history[\"val_acc\"]\n",
    "validating_loss = history.history[\"val_loss\"]\n",
    "last_5_epoch_val_acc = validating_acc[5:]\n",
    "\n",
    "print(np.mean(validating_loss))\n",
    "print(np.average(last_5_epoch_val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "What is the mean of test loss for all the epochs for the model trained with augmentations?\n",
    "\n",
    "* 0.18\n",
    "* 0.48\n",
    "* 0.78\n",
    "* 0.108\n",
    "\n",
    "### Answer to Q5:\n",
    "- 0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "What's the average of test accuracy for the last 5 epochs (from 6 to 10)\n",
    "for the model trained with augmentations?\n",
    "* 0.38\n",
    "* 0.58\n",
    "* 0.78\n",
    "* 0.98\n",
    "\n",
    "### Answer to Q6:\n",
    "- 0.78"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoomcamp-9FM8Shbn-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
