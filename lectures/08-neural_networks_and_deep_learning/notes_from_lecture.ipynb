{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 299, 3)\n",
      "[[[179 171  99]\n",
      "  [179 171  99]\n",
      "  [181 173 101]\n",
      "  ...\n",
      "  [251 253 248]\n",
      "  [251 253 248]\n",
      "  [251 254 247]]\n",
      "\n",
      " [[188 179 112]\n",
      "  [187 178 111]\n",
      "  [186 177 108]\n",
      "  ...\n",
      "  [251 252 247]\n",
      "  [251 252 247]\n",
      "  [251 252 246]]\n",
      "\n",
      " [[199 189 127]\n",
      "  [200 190 128]\n",
      "  [200 191 126]\n",
      "  ...\n",
      "  [250 251 245]\n",
      "  [250 251 245]\n",
      "  [250 251 245]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[165 151  76]\n",
      "  [173 159  84]\n",
      "  [171 157  82]\n",
      "  ...\n",
      "  [183 135  25]\n",
      "  [181 133  22]\n",
      "  [183 135  24]]\n",
      "\n",
      " [[165 151  76]\n",
      "  [173 159  84]\n",
      "  [171 157  82]\n",
      "  ...\n",
      "  [182 134  23]\n",
      "  [180 132  21]\n",
      "  [182 134  23]]\n",
      "\n",
      " [[165 151  76]\n",
      "  [173 159  84]\n",
      "  [171 157  82]\n",
      "  ...\n",
      "  [181 133  22]\n",
      "  [179 131  20]\n",
      "  [182 134  23]]]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../../clothing-dataset-small/train/t-shirt/5f0a3fa0-6a3d-4b68-b213-72766a643de7.jpg\"\n",
    "# Neural network expects image of certain size (usually 299^2, 224^2 or 150^2)\n",
    "img = load_img(file_path, target_size=(299, 299))\n",
    "# Uses PIL\n",
    "# IMG consists is an array that has 3 channels (R, G, B) - each channel contains a representation of pixels with the appropriate value from this channel\n",
    "# Images are encoded internally - array of image size and 3 channels - e.g. (150, 150, 3)\n",
    "# Can transform PIL image into Numpy array, where each row is a pixel - [[[177 169  97], and we have 15 of them\n",
    "x = np.array(img)\n",
    "print(x.shape)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Pre-trained convolutional neural networks\n",
    "- Imagenet dataset: https://www.image-net.org/\n",
    "- Pre-trained models: https://keras.io/api/applications/\n",
    "- Using SaturnCloud for running models in cloud - with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.applications.xception import decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Xception(weights=\"imagenet\", input_shape=(299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 299, 299, 3)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([x])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.4039216 ,  0.3411765 , -0.2235294 ],\n",
       "        [ 0.4039216 ,  0.3411765 , -0.2235294 ],\n",
       "        [ 0.41960788,  0.35686278, -0.20784312],\n",
       "        ...,\n",
       "        [ 0.96862745,  0.9843137 ,  0.94509804],\n",
       "        [ 0.96862745,  0.9843137 ,  0.94509804],\n",
       "        [ 0.96862745,  0.99215686,  0.9372549 ]],\n",
       "\n",
       "       [[ 0.47450984,  0.4039216 , -0.12156862],\n",
       "        [ 0.4666667 ,  0.39607847, -0.12941176],\n",
       "        [ 0.45882356,  0.38823533, -0.15294117],\n",
       "        ...,\n",
       "        [ 0.96862745,  0.9764706 ,  0.9372549 ],\n",
       "        [ 0.96862745,  0.9764706 ,  0.9372549 ],\n",
       "        [ 0.96862745,  0.9764706 ,  0.92941177]],\n",
       "\n",
       "       [[ 0.56078434,  0.48235297, -0.00392157],\n",
       "        [ 0.5686275 ,  0.4901961 ,  0.00392163],\n",
       "        [ 0.5686275 ,  0.49803925, -0.01176471],\n",
       "        ...,\n",
       "        [ 0.9607843 ,  0.96862745,  0.92156863],\n",
       "        [ 0.9607843 ,  0.96862745,  0.92156863],\n",
       "        [ 0.9607843 ,  0.96862745,  0.92156863]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.2941177 ,  0.18431377, -0.40392154],\n",
       "        [ 0.35686278,  0.24705887, -0.34117645],\n",
       "        [ 0.3411765 ,  0.2313726 , -0.35686272],\n",
       "        ...,\n",
       "        [ 0.43529415,  0.05882359, -0.8039216 ],\n",
       "        [ 0.41960788,  0.04313731, -0.827451  ],\n",
       "        [ 0.43529415,  0.05882359, -0.8117647 ]],\n",
       "\n",
       "       [[ 0.2941177 ,  0.18431377, -0.40392154],\n",
       "        [ 0.35686278,  0.24705887, -0.34117645],\n",
       "        [ 0.3411765 ,  0.2313726 , -0.35686272],\n",
       "        ...,\n",
       "        [ 0.427451  ,  0.05098045, -0.81960785],\n",
       "        [ 0.41176474,  0.03529418, -0.8352941 ],\n",
       "        [ 0.427451  ,  0.05098045, -0.81960785]],\n",
       "\n",
       "       [[ 0.2941177 ,  0.18431377, -0.40392154],\n",
       "        [ 0.35686278,  0.24705887, -0.34117645],\n",
       "        [ 0.3411765 ,  0.2313726 , -0.35686272],\n",
       "        ...,\n",
       "        [ 0.41960788,  0.04313731, -0.827451  ],\n",
       "        [ 0.4039216 ,  0.02745104, -0.84313726],\n",
       "        [ 0.427451  ,  0.05098045, -0.81960785]]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always need to use `preprocess_input`\n",
    "X = preprocess_input(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 572ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('n03595614', 'jersey', 0.68196297),\n",
       "  ('n02916936', 'bulletproof_vest', 0.03814007),\n",
       "  ('n04370456', 'sweatshirt', 0.034324836),\n",
       "  ('n03710637', 'maillot', 0.011354229),\n",
       "  ('n04525038', 'velvet', 0.0018453625)]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_predictions(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. Convolutional neural networks\n",
    "- Types of layers\n",
    "- Convolutional layers and filters\n",
    "- Dense layers\n",
    "- Mostly used for images\n",
    "  \n",
    "There are more layers, read: https://cs231n.github.io\n",
    "\n",
    "\n",
    "1. Convolutional (role - extract vector representation of the image)\n",
    "   Consists of filters, small images,e.g. 5x5. Contains simple shapes, lines.\n",
    "   Everytime the filters i applied we slide the filter across the image - and calculate similarity between the filter and the image. - We get Feature map where highher value in a \"cell\" means higher similarity with the filter.\n",
    "   Input - image, output - feature maps (one for each filter).\n",
    "   Ouptut of 1st convolutional layer is a set of feature map. \n",
    "   Then we use this as an image to the 2nd convolutional layer.\n",
    "   .. could be more convolutional layers and each chained actions (layer , filters, feature maps) learns more complex filters.\n",
    "\n",
    "   Filters are \"learned\" during training.\n",
    "   e.g. 3 layers\n",
    "   1st layer learns simple filters (e.g. simple shapes)\n",
    "   2nd layer learns more complex shapes (by combining filters from previous layers)\n",
    "   3rd layer learns even more complex shapes\n",
    "   Finaly, there's a vectore representation that' s ready to be used in dense layer\n",
    "2. Dense layers (role - make final prediction)\n",
    "   Dense - because each elem of input connects to each elem of output.\n",
    "   So it's essentially matrix multiplication\n",
    "   E.g. if we want to predict if image is a t-shirt, dress or jeans, we could use Logistic regression.\n",
    "   Multiplicating the iumage vector values xi with the specific class weights wj.\n",
    "   Possible to combine multiple dense layers\n",
    "\n",
    "* Pooling layer \n",
    "  Takes feature map that one of Convolutional layers learned and it makes it smaller. Forcing it to have fewer parameters.\n",
    "  E.g. if feature map os 200x200, after pooling it could be 100x100.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. Transfer learning\n",
    "- Reading data with ImageDataGenerator\n",
    "- Train `Xception` on smaller images (150x150)\n",
    "- Better run this on GPU (using Saturn Cloud)\n",
    "\n",
    "\n",
    "- Transforming Image to Vector and learning the filters is quite generic, no need to change that\n",
    "- Specific to the ImageNet dataset  - dense layers\n",
    "- So we keep the convolutional layers but train new dense layers (the idea behind transfer learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = train_gen.flow_from_directory(\n",
    "    '../../clothing-dataset-small/train',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 264\n",
      "drwxrwxr-x 2 tom tom 20480 nov  9 20:21 dress\n",
      "drwxrwxr-x 2 tom tom 12288 nov  9 20:21 hat\n",
      "drwxrwxr-x 2 tom tom 36864 nov  9 20:21 longsleeve\n",
      "drwxrwxr-x 2 tom tom 20480 nov  9 20:21 outwear\n",
      "drwxrwxr-x 2 tom tom 36864 nov  9 20:21 pants\n",
      "drwxrwxr-x 2 tom tom 20480 nov  9 20:21 shirt\n",
      "drwxrwxr-x 2 tom tom 20480 nov  9 20:21 shoes\n",
      "drwxrwxr-x 2 tom tom 20480 nov  9 20:21 shorts\n",
      "drwxrwxr-x 2 tom tom 12288 nov  9 20:21 skirt\n",
      "drwxrwxr-x 2 tom tom 69632 nov  9 20:21 t-shirt\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../../clothing-dataset-small/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': 0,\n",
       " 'hat': 1,\n",
       " 'longsleeve': 2,\n",
       " 'outwear': 3,\n",
       " 'pants': 4,\n",
       " 'shirt': 5,\n",
       " 'shoes': 6,\n",
       " 'shorts': 7,\n",
       " 'skirt': 8,\n",
       " 't-shirt': 9}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class names are inferred from folder names\n",
    "#  e.g. everything inside t-shirt folder was put under 't-shirt' class\n",
    "train_ds.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]  # E.g. last column represents t-shirt, using one-hot encoding for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 341 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_ds = val_gen.flow_from_directory(\n",
    "    '../../clothing-dataset-small/validation',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model will be the convolutional layer\n",
    "# we will train custom model on top of that\n",
    "## include_top=False - in Keras mentally arrange the visual represenation of deep learning from bottom to top,\n",
    "# where bottom - convolutional layer and top - dense layer and prediction\n",
    "base_model = Xception(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(150,150,3),\n",
    ")\n",
    "\n",
    "# trainable = False - meaning when we train our model, we don't want to change convolutional layers\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a new top\n",
    "\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "\n",
    "base = base_model(inputs)\n",
    "\n",
    "# Averages out a single filter (5x5) to a single value ,so we can get a vector repr of an image \n",
    "vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
    "\n",
    "# Dimensionality is 10, because we have 10 classes\n",
    "outputs = keras.layers.Dense(10)(vectors)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 10)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer - calculates weights by checking whether chaneg in wiegth leads to better outcome (by evaluating 'loss')\n",
    "# there are different optimizers\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "loss = keras.losses.CategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoomcamp-9FM8Shbn-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
